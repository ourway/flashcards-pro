[
  {
    "question": "Can you perform Data Manipulation Language (DML) operations like INSERT, UPDATE, or DELETE on Iceberg tables using PySpark *through Trino*? If so, provide a PySpark code example for inserting data.",
    "answer": "Yes, you can perform DML operations on Iceberg tables through Trino using PySpark, although it's less common than direct Spark writes. Trino acts as a SQL engine to interact with Iceberg, and PySpark can send DML SQL commands to Trino. Here's an example of inserting data:\n\n```python markdown\nfrom pyspark.sql import SparkSession\n\nspark = # ... (SparkSession configuration) ... getOrCreate()\n\n# Create a DataFrame to insert\ndata = [{\"order_id\": 1001, \"customer_id\": \"CUST001\", \"order_date\": \"2023-11-15\", \"order_total\": 150.00},\n        {\"order_id\": 1002, \"customer_id\": \"CUST002\", \"order_date\": \"2023-11-16\", \"order_total\": 220.50}]\ndf_to_insert = spark.createDataFrame(data)\n\n# Insert data into Iceberg table via Trino INSERT SQL\ntrino_insert_sql = f\"\"\"\\nINSERT INTO your_trino_catalog.your_iceberg_schema.my_iceberg_table (order_id, customer_id, order_date, order_total)\\nVALUES {', '.join([f'({row['order_id']}, \\'{row['customer_id']}\\', DATE \\'{row['order_date']}\\', {row['order_total']})' for row in data])}\\n\"\"\"\n\nspark.sql(trino_insert_sql)\n\n# Verify insertion (optional)\ndf_verify = spark.sql(\"\"\"SELECT * FROM your_trino_catalog.your_iceberg_schema.my_iceberg_table WHERE order_id IN (1001, 1002)\"\"\" )\ndf_verify.show()\n\nprint(\"Trino INSERT SQL Query executed:\\n\", trino_insert_sql)\n\nspark.stop()\n\n-- Explanation:\n-- 1. We create a PySpark DataFrame (`df_to_insert`) with the data to be inserted.\n-- 2. We construct a Trino `INSERT INTO` SQL query string (`trino_insert_sql`).\n--    - The query explicitly lists column names and uses `VALUES` clause to insert rows.\n--    - String formatting is used to build the `VALUES` part from the DataFrame data.\\n--    - Note the use of `DATE '{row['order_date']}'` for date literals in Trino SQL.\\n-- 3. `spark.sql(trino_insert_sql)` sends the INSERT command to Trino for execution.\n-- 4. (Optional) We verify the insertion by querying back the inserted data.\n-- While INSERT is possible, direct PySpark Dataframe `write` operations to Iceberg\n-- (using Iceberg Spark connector) are often more performant for bulk data loading.\n-- Replace placeholders with your actual catalog, schema, and table names.\n```\n\nThis example demonstrates INSERT operations.  UPDATE and DELETE operations can be performed similarly using Trino's SQL syntax within `spark.sql()`. However, for large-scale data ingestion and modification, using Spark's native Iceberg connector for direct writes to storage is generally more efficient than routing DML through Trino.",
    "reference": "https://trino.io/docs/current/sql/insert.html (Trino INSERT syntax), https://iceberg.apache.org/docs/latest/spark-writes/ (Iceberg Spark Writes - for comparison)"
  },
  {
    "question": "Demonstrate a more complex predicate pushdown scenario in PySpark querying Iceberg via Trino. Use multiple filter conditions, including a function in the filter. Verify pushdown.",
    "answer": "Let's illustrate predicate pushdown with a more complex filter, including multiple conditions and a function, and verify that the filter is effectively pushed down to Trino.\n\n```python markdown\nfrom pyspark.sql import SparkSession\n\nspark = # ... (SparkSession configuration) ... getOrCreate()\n\n# Complex filter with multiple conditions and a function (YEAR)\ndf_complex_filter = spark.sql(\"\"\"\nSELECT \n    *\nFROM \n    your_trino_catalog.your_iceberg_schema.my_partitioned_iceberg_table\nWHERE \n    event_category = 'login' \n    AND event_status IN ('success', 'failure')\n    AND YEAR(event_timestamp) = 2023\n\"\"\")\n\ndf_complex_filter.explain(extended=True)\n# df_complex_filter.show()\n\nspark.stop()\n\n-- Explanation:\n-- 1.  The `WHERE` clause now has multiple conditions:\n--    - `event_category = 'login'` (equality filter)\n--    - `event_status IN ('success', 'failure')` (IN filter)\n--    - `YEAR(event_timestamp) = 2023` (function in filter).\n-- 2.  Examine `df_complex_filter.explain(extended=True)` physical plan output.\\n--    - Look for filter operators applied *early* in the plan, ideally close to\n--      the `DataSourceScan`. You should see *all* filter conditions (including the\n--      YEAR function condition) being part of the filter expression in the plan.\\n-- 3.  Check Trino query logs (if possible) to confirm the generated Trino SQL\n--    also includes these complex filters in the `WHERE` clause, indicating\n--    successful predicate pushdown.\n\n# Expected Physical Plan snippet (indicative, may vary):\n# ...\n# +- *Filter [... AND ... AND ... (year(event_timestamp#...) = 2023)]  <-- Complex filter in Spark plan\n#    +- DataSourceScan [...] \n# ...\n\n# Trino Query Log verification example:\n# SELECT ...\n# WHERE event_category = 'login' AND event_status IN ('success', 'failure') AND year(event_timestamp) = 2023\n# ... FROM iceberg.your_iceberg_schema.my_partitioned_iceberg_table\n```\n\nThis example demonstrates that predicate pushdown with Trino and Iceberg is not limited to simple filters. Trino can often push down more complex predicates, including those involving functions, enhancing query efficiency. Verification via `explain()` and Trino logs is key to confirm pushdown behavior.",
    "reference": "https://iceberg.apache.org/spec#filtering (Iceberg Filtering), Trino and Spark connector documentation on predicate pushdown capabilities and function support"
  },
  {
    "question": "Demonstrate a PySpark query (via Trino) to compare data in an Iceberg table between two different points in time using Time Travel for day-over-day analysis.",
    "answer": "Time Travel is powerful for comparing data across different versions. Here's a PySpark example (via Trino) to compare data from 'today' versus 'yesterday' for a day-over-day analysis:\n\n```python markdown\nfrom pyspark.sql import SparkSession\nfrom datetime import datetime, timedelta\n\nspark = # ... (SparkSession configuration) ... getOrCreate()\n\ntoday_ts = datetime.utcnow()\nyesterday_ts = today_ts - timedelta(days=1)\n\n# Time Travel queries for today and yesterday\ntrino_sql_today = f\"\"\"\nSELECT \n    order_id, customer_id, order_total\nFROM \n    your_trino_catalog.your_iceberg_schema.my_iceberg_table\nFOR SYSTEM_TIME AS OF TIMESTAMP '{today_ts.strftime('%Y-%m-%d %H:%M:%S UTC')}'\n\"\"\"\ntrino_sql_yesterday = f\"\"\"\nSELECT \n    order_id, customer_id, order_total\nFROM \n    your_trino_catalog.your_iceberg_schema.my_iceberg_table\nFOR SYSTEM_TIME AS OF TIMESTAMP '{yesterday_ts.strftime('%Y-%m-%d %H:%M:%S UTC')}'\n\"\"\"\n\ndf_today = spark.sql(trino_sql_today).withColumnRenamed(\"order_total\", \"order_total_today\")\ndf_yesterday = spark.sql(trino_sql_yesterday).withColumnRenamed(\"order_total\", \"order_total_yesterday\")\n\n# Join and compare order totals\nddf_comparison = df_today.join(df_yesterday, on=[\"order_id\", \"customer_id\"], how=\"fullouter\")\\\n    .select(\"order_id\", \"customer_id\", \"order_total_today\", \"order_total_yesterday\",\n            (df_today[\"order_total_today\"] - df_yesterday[\"order_total_yesterday\"]).alias(\"order_total_diff\"))\n\ndf_comparison.show()\n\nspark.stop()\n\n-- Explanation:\n-- 1. We define timestamps for 'today' and 'yesterday' using `datetime` and `timedelta`.\n-- 2. Two Trino SQL queries are constructed, each using `FOR SYSTEM_TIME AS OF` to query\n--    the Iceberg table at 'today_ts' and 'yesterday_ts' respectively.\n-- 3. We rename the 'order_total' column in each DataFrame to distinguish them ('order_total_today', 'order_total_yesterday').\n-- 4. A `fullouter` join is performed on 'order_id' and 'customer_id' to combine data from both time points.\n-- 5. A new column 'order_total_diff' is calculated to show the difference in order totals between today and yesterday.\n-- This example demonstrates how Time Travel facilitates temporal data analysis, comparing table states across different points in time using PySpark and Trino.\n-- Remember to replace placeholders and adjust timestamps and columns as needed.\n```\n\nThis code showcases day-over-day analysis using Time Travel. By querying the table at two different timestamps and joining the results, you can effectively compare data evolution over time. This is a powerful application of Iceberg's versioning capabilities.",
    "reference": "https://iceberg.apache.org/spec#snapshots (Iceberg Snapshots), Trino documentation on Time Travel, PySpark DataFrame joins"
  },
  {
    "question": "How would you perform Schema Evolution on an Iceberg table using PySpark when connected via Trino? Specifically, demonstrate adding a new column named 'customer_segment' of type STRING.",
    "answer": "Schema Evolution operations are typically performed using SQL DDL commands. When using PySpark with Trino, you can execute Trino's `ALTER TABLE` statements for schema evolution.\n\n```python markdown\nfrom pyspark.sql import SparkSession\n\nspark = # ... (SparkSession configuration) ... getOrCreate()\n\nnew_column_name = "customer_segment"\n\n# Trino SQL to add a new column 'customer_segment' of type STRING\ntrino_alter_table_sql = f\"\"\"\nALTER TABLE your_trino_catalog.your_iceberg_schema.my_iceberg_table\nADD COLUMN {new_column_name} VARCHAR\n\"\"\"\n\nspark.sql(trino_alter_table_sql)\n\nprint(\"Trino ALTER TABLE SQL Query executed for schema evolution:\\n\", trino_alter_table_sql)\n\n# Verify schema change (optional) - query schema history\ndf_schema_history = spark.sql(\"\"\"SELECT schema_id, schema FROM your_trino_catalog.your_iceberg_schema.my_iceberg_table$schemas ORDER BY schema_id DESC LIMIT 2\"\"\" )\ndf_schema_history.show(truncate=False)\n\nspark.stop()\n\n-- Explanation:\n-- 1. We construct a Trino `ALTER TABLE ... ADD COLUMN ...` SQL query string.\n--    -  `ALTER TABLE ... ADD COLUMN ...` is the standard Trino DDL for adding columns.\n-- 2. `spark.sql(trino_alter_table_sql)` executes this DDL command on Trino, which in turn\n--    performs the schema evolution on the Iceberg table's metadata.\n-- 3. (Optional) We verify the schema change by querying the `$schemas` system table\n--    to see the updated schema history. The latest schema should include the\n--    new 'customer_segment' column.\n-- Note: Schema evolution operations are metadata-only operations in Iceberg;\n-- they do not rewrite existing data files. New data written after schema evolution\n-- will include the new column, while older data will effectively have NULL for the new column.\n-- Replace placeholders with your actual catalog, schema, and table names.\n```\n\nThis example shows how to perform schema evolution (adding a column) using Trino SQL commands executed from PySpark. Iceberg's schema evolution is metadata-driven and efficient, allowing schema changes without data rewriting. See [Iceberg Schema Evolution](https://iceberg.apache.org/spec#schema-evolution) for more details on schema alteration operations.",
    "reference": "https://iceberg.apache.org/spec#schema-evolution (Iceberg Schema Evolution), Trino documentation on ALTER TABLE: https://trino.io/docs/current/sql/alter-table.html"
  },
  {
    "question": "In a PySpark application querying Iceberg through Trino on EMR, what are some key performance considerations and best practices to optimize query speed and efficiency?",
    "answer": "When using PySpark to query Iceberg tables via Trino on EMR, consider these performance best practices:\n\n- **Predicate Pushdown:**  Leverage Iceberg and Trino's predicate pushdown capabilities by filtering data as early as possible in your queries (using `WHERE` clauses). Ensure filters are actually pushed down by examining query plans (`df.explain()`).\n- **Column Projection:** Select only the columns you need in your queries. Avoid `SELECT *` on wide tables. Iceberg's column projection ensures only necessary data is read.\n- **Partition Filtering:** For partitioned tables, always filter on partition columns in your `WHERE` clause to prune unnecessary partitions. This is crucial for large partitioned tables.\n- **Data Locality:** Ensure your EMR cluster and Trino cluster are located in the same AWS region as your S3 data lake for optimal data transfer speeds.\n- **File Format Optimization:** Use efficient columnar file formats like Parquet for your Iceberg tables. Parquet is highly recommended for analytical workloads.\n- **Trino and EMR Configuration:** Properly configure your Trino and EMR clusters. Optimize Trino's memory settings, worker node counts, and Iceberg connector configurations. Tune Spark executor memory and cores for your workload.\n- **Data Compaction and Metadata Compaction:** Regularly perform Iceberg data compaction to optimize data file sizes and metadata compaction to keep metadata efficient. This improves query planning and data access.\n- **Caching (if applicable):**  Consider Trino's caching mechanisms or Spark caching strategically if you have repetitive queries or intermediate results that can benefit from caching.\n- **Avoid Unnecessary Data Transfer:** Minimize data transfer between Trino and Spark. If possible, perform as much data processing as possible within Trino (using Trino's SQL capabilities) before transferring data to PySpark.\n- **Monitoring and Profiling:** Monitor your Trino and Spark query performance. Use profiling tools to identify bottlenecks and areas for optimization.\n\nBy applying these best practices, you can significantly improve the performance and efficiency of your PySpark applications querying Iceberg tables through Trino on EMR.",
    "reference": "https://iceberg.apache.org/spec (Iceberg spec generally for performance features like partitioning, filtering), Trino documentation on performance tuning, Spark performance tuning guides"
  }
]
