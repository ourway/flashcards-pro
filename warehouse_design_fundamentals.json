[
  {
    "question": "What is the Kimball approach to Data Warehousing?",
    "answer": "Kimball promotes a bottom-up approach where data marts are built using Star or Snowflake schemas and later integrated into a data warehouse.\n\nCase Study: Netflix uses Kimball’s Dimensional Modeling to optimize customer viewing data for fast analytics, improving personalized recommendations.\n\nReal-Time Architecture: Netflix combines batch ETL using Apache Spark with a real-time data pipeline using Apache Flink to personalize content recommendations instantly.",
    "reference": "https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/"
  },
  {
    "question": "What is the Inmon approach to Data Warehousing?",
    "answer": "Inmon promotes a top-down approach where a normalized enterprise data warehouse (EDW) is created first, and then data marts are built for specific business functions.\n\nCase Study: Bank of America follows Inmon's approach for a centralized enterprise data warehouse that supports compliance and regulatory reporting.\n\nReal-Time Architecture: Bank of America integrates real-time fraud detection using Kafka and Flink while maintaining a structured EDW in Oracle.",
    "reference": "https://www.inmon.com/"
  },
  {
    "question": "What is a Star Schema in data modeling?",
    "answer": "A Star Schema consists of a central Fact table linked to multiple Dimension tables. It simplifies queries and improves performance by reducing joins.\n\nCase Study: Walmart uses Star Schema to track sales data efficiently, enabling rapid aggregation of transaction details for BI reporting.\n\nReal-Time Architecture: Walmart streams real-time sales data into its data lake via Kafka, later transforming it using dbt before storing it in a Redshift Star Schema.",
    "reference": "https://www.dataversity.net/star-schema-definition/"
  },
  {
    "question": "What is a Snowflake Schema, and how does it differ from Star Schema?",
    "answer": "A Snowflake Schema normalizes dimension tables by breaking them into sub-dimensions, reducing redundancy but increasing joins compared to a Star Schema.\n\nCase Study: Amazon uses Snowflake Schema to manage their product catalog efficiently, reducing storage duplication across millions of SKUs.\n\nReal-Time Architecture: Amazon DynamoDB streams new products into Snowflake for further transformation, maintaining a real-time updated product catalog.",
    "reference": "https://www.datawarehouse4u.info/Snowflake-Schema.html"
  },
  {
    "question": "What is a Fact Table in a Data Warehouse?",
    "answer": "A Fact Table stores business events (transactions) and contains foreign keys linking to Dimension Tables, along with numeric measures (e.g., sales amount, quantity).\n\nCase Study: Uber’s ride transactions are stored in a Fact Table with dimensions for drivers, passengers, and ride duration to enable surge pricing analytics.\n\nReal-Time Architecture: Uber uses Apache Kafka to stream ride events into their Fact Table in a data warehouse while running real-time analytics in Presto.",
    "reference": "https://www.dataversity.net/fact-table-definition/"
  },
  {
    "question": "What is a Slowly Changing Dimension (SCD)?",
    "answer": "A Slowly Changing Dimension (SCD) manages historical changes in dimension data.\n- SCD Type 1: Overwrites old data.\n- SCD Type 2: Creates new rows for changes.\n- SCD Type 3: Stores a limited history using extra columns.\n\nCase Study: A global bank tracks customer address changes using SCD Type 2 to maintain historical records for fraud detection.\n\nReal-Time Architecture: Banks use Change Data Capture (CDC) with Debezium to track SCD changes in real-time and sync them with cloud-based data warehouses.",
    "reference": "https://www.sqlshack.com/slowly-changing-dimension-scd-types/"
  },
  {
    "question": "What is Data Vault Modeling, and how does it differ from Kimball and Inmon?",
    "answer": "Data Vault is a hybrid model that combines normalization (like Inmon) and dimensional modeling (like Kimball). It separates business keys (Hubs), relationships (Links), and attributes (Satellites) to support scalability and historical tracking.\n\nCase Study: Airbnb uses Data Vault to integrate structured and semi-structured booking data across multiple regions while maintaining historical integrity.\n\nReal-Time Architecture: Airbnb streams booking transactions via Kafka and stores them in Data Vault models for historical tracking and fraud detection.",
    "reference": "https://www.datavaultacademy.com/what-is-data-vault/"
  },
  {
    "question": "What are the best practices for designing a Data Warehouse?",
    "answer": "- Clearly define business requirements\n- Use appropriate schema (Star/Snowflake)\n- Optimize queries with indexing and partitioning\n- Implement Slowly Changing Dimensions for historical tracking\n- Ensure security and compliance with access controls\n\nCase Study: Tesla optimizes sensor data warehousing by partitioning time-series vehicle performance data, improving analytics for predictive maintenance.\n\nReal-Time Architecture: Tesla ingests real-time vehicle telemetry data into a time-series data warehouse, enabling predictive analytics for maintenance alerts.",
    "reference": "https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/"
  },
  {
    "question": "What is an OLAP Cube, and when should you use it?",
    "answer": "OLAP Cubes pre-aggregate data for fast multidimensional analysis, making them ideal for high-performance analytical queries in BI tools.\n\nCase Study: Microsoft uses OLAP cubes in Azure Analysis Services for real-time financial reporting.\n\nReal-Time Architecture: Financial institutions use OLAP cubes combined with real-time event streaming from Apache Kafka to track high-frequency trades instantly.",
    "reference": "https://www.techopedia.com/definition/27998/online-analytical-processing-olap"
  },
  {
    "question": "How does real-time data streaming enhance data warehousing?",
    "answer": "Real-time data streaming (via Kafka, Flink, or Spark Streaming) ensures that data warehouses receive live updates instead of relying solely on batch ETL.\n\nCase Study: Twitter processes billions of real-time user events per day and streams them into Snowflake for near-instant analytics.\n\nReal-Time Architecture: Twitter uses Kafka Streams for real-time data ingestion and Snowflake for storage and analytics, reducing ETL lag.",
    "reference": "https://flink.apache.org/"
  }
]
