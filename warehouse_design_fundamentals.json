[
  {
    "question": "What is the Kimball approach to Data Warehousing?",
    "answer": "Kimball promotes a bottom-up approach where data marts are built using Star or Snowflake schemas and later integrated into a data warehouse.\n\n### What are Data Marts?\nData marts are specialized databases that store domain-specific data optimized for analysis. They are created for departments like Sales, HR, and Finance before integrating into an Enterprise Data Warehouse (EDW).\n\n### Example Data Mart Structure:\n| Data Mart | Purpose |\n|-----------|---------|\n| Sales Mart | Stores sales transactions, products, and customer purchases |\n| HR Mart | Stores employee records, payroll, and department structure |\n| Finance Mart | Stores accounts, transactions, and financial reporting data |\n\n### Architecture Diagram:\n```plaintext\n  Sales Mart     HR Mart     Finance Mart\n      |             |              |\n      ----------------------------------\n                  |\n         Enterprise Data Warehouse (EDW)\n```\n\n### Case Study:\nNetflix uses Kimball’s Dimensional Modeling to optimize customer viewing data for fast analytics, improving personalized recommendations.\n\n### Real-Time Architecture:\nNetflix combines batch ETL using Apache Spark with a real-time data pipeline using Apache Flink to personalize content recommendations instantly.",
    "reference": "https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/"
  },
  {
    "question": "What is the Inmon approach to Data Warehousing?",
    "answer": "Inmon promotes a top-down approach where a normalized enterprise data warehouse (EDW) is created first, and then data marts are built for specific business functions.\n\n### Key Differences Between Kimball and Inmon:\n| Approach | Kimball | Inmon |\n|----------|--------|-------|\n| Data Structure | Denormalized (Star/Snowflake Schema) | Normalized (3NF) |\n| Integration | Data Marts first, then integrated | Central EDW first, then Data Marts |\n| Query Performance | Optimized for fast querying | Optimized for data integrity and flexibility |\n\n### Example Pseudocode for a Normalized EDW:\n```sql\nCREATE TABLE Customers (\n    CustomerID INT PRIMARY KEY,\n    Name VARCHAR(100),\n    Region VARCHAR(50)\n);\n\nCREATE TABLE Transactions (\n    TransactionID INT PRIMARY KEY,\n    CustomerID INT,\n    Amount DECIMAL(10,2),\n    Date DATE,\n    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)\n);\n```\n\n### Case Study:\nBank of America follows Inmon's approach for a centralized enterprise data warehouse that supports compliance and regulatory reporting.\n\n### Real-Time Architecture:\nBank of America integrates real-time fraud detection using Kafka and Flink while maintaining a structured EDW in Oracle.",
    "reference": "https://www.inmon.com/"
  },
  {
    "question": "What is a Star Schema in data modeling?",
    "answer": "A Star Schema consists of a central Fact table linked to multiple Dimension tables. It simplifies queries and improves performance by reducing joins.\n\n### Star Schema Diagram:\n```plaintext\n          +------------------+\n          |  Time Dimension  |\n          +------------------+\n                  |\n+--------------+       +----------------+\n| Product Dim  |------>|  Sales Fact    |<------| Customer Dim |\n+--------------+       +----------------+\n```\n\n### Example SQL:\n```sql\nCREATE TABLE Sales_Fact (\n    SaleID INT PRIMARY KEY,\n    ProductID INT,\n    CustomerID INT,\n    TimeID INT,\n    Amount DECIMAL(10,2)\n);\n```\n\n### Case Study:\nWalmart uses Star Schema to track sales data efficiently, enabling rapid aggregation of transaction details for BI reporting.\n\n### Real-Time Architecture:\nWalmart streams real-time sales data into its data lake via Kafka, later transforming it using dbt before storing it in a Redshift Star Schema.",
    "reference": "https://www.dataversity.net/star-schema-definition/"
  },
  {
    "question": "What is a Snowflake Schema, and how does it differ from Star Schema?",
    "answer": "A Snowflake Schema normalizes dimension tables by breaking them into sub-dimensions, reducing redundancy but increasing joins compared to a Star Schema.\n\nCase Study: Amazon uses Snowflake Schema to manage their product catalog efficiently, reducing storage duplication across millions of SKUs.\n\nReal-Time Architecture: Amazon DynamoDB streams new products into Snowflake for further transformation, maintaining a real-time updated product catalog.",
    "reference": "https://www.datawarehouse4u.info/Snowflake-Schema.html"
  },
  {
    "question": "What is a Fact Table in a Data Warehouse?",
    "answer": "A Fact Table stores business events (transactions) and contains foreign keys linking to Dimension Tables, along with numeric measures (e.g., sales amount, quantity).\n\nCase Study: Uber’s ride transactions are stored in a Fact Table with dimensions for drivers, passengers, and ride duration to enable surge pricing analytics.\n\nReal-Time Architecture: Uber uses Apache Kafka to stream ride events into their Fact Table in a data warehouse while running real-time analytics in Presto.",
    "reference": "https://www.dataversity.net/fact-table-definition/"
  },
  {
    "question": "What is a Slowly Changing Dimension (SCD)?",
    "answer": "A Slowly Changing Dimension (SCD) manages historical changes in dimension data.\n- SCD Type 1: Overwrites old data.\n- SCD Type 2: Creates new rows for changes.\n- SCD Type 3: Stores a limited history using extra columns.\n\nCase Study: A global bank tracks customer address changes using SCD Type 2 to maintain historical records for fraud detection.\n\nReal-Time Architecture: Banks use Change Data Capture (CDC) with Debezium to track SCD changes in real-time and sync them with cloud-based data warehouses.",
    "reference": "https://www.sqlshack.com/slowly-changing-dimension-scd-types/"
  },
  {
    "question": "What is Data Vault Modeling, and how does it differ from Kimball and Inmon?",
    "answer": "Data Vault is a hybrid model that combines normalization (like Inmon) and dimensional modeling (like Kimball). It separates business keys (Hubs), relationships (Links), and attributes (Satellites) to support scalability and historical tracking.\n\nCase Study: Airbnb uses Data Vault to integrate structured and semi-structured booking data across multiple regions while maintaining historical integrity.\n\nReal-Time Architecture: Airbnb streams booking transactions via Kafka and stores them in Data Vault models for historical tracking and fraud detection.",
    "reference": "https://www.datavaultacademy.com/what-is-data-vault/"
  },
  {
    "question": "What are the best practices for designing a Data Warehouse?",
    "answer": "- Clearly define business requirements\n- Use appropriate schema (Star/Snowflake)\n- Optimize queries with indexing and partitioning\n- Implement Slowly Changing Dimensions for historical tracking\n- Ensure security and compliance with access controls\n\nCase Study: Tesla optimizes sensor data warehousing by partitioning time-series vehicle performance data, improving analytics for predictive maintenance.\n\nReal-Time Architecture: Tesla ingests real-time vehicle telemetry data into a time-series data warehouse, enabling predictive analytics for maintenance alerts.",
    "reference": "https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/"
  },
  {
    "question": "What is an OLAP Cube, and when should you use it?",
    "answer": "OLAP Cubes pre-aggregate data for fast multidimensional analysis, making them ideal for high-performance analytical queries in BI tools.\n\n### OLAP Cube Structure:\n| Dimension | Hierarchy |\n|-----------|----------|\n| Time | Year → Quarter → Month → Day |\n| Location | Country → Region → City |\n| Product | Category → Subcategory → Product Name |\n\n### Querying OLAP Cubes:\n```sql\nSELECT SUM(sales_amount)\nFROM OLAP_Cube\nWHERE Year = 2023 AND Product_Category = 'Electronics';\n```\n\n### Case Study:\nMicrosoft uses OLAP cubes in Azure Analysis Services for real-time financial reporting.\n\n### Real-Time Architecture:\nFinancial institutions use OLAP cubes combined with real-time event streaming from Apache Kafka to track high-frequency trades instantly.",
    "reference": "https://www.techopedia.com/definition/27998/online-analytical-processing-olap"
  },
  {
    "question": "How does real-time data streaming enhance data warehousing?",
    "answer": "Real-time data streaming (via Kafka, Flink, or Spark Streaming) ensures that data warehouses receive live updates instead of relying solely on batch ETL.\n\n### Example Real-Time Streaming Architecture:\n```plaintext\n  +---------+       +--------+      +--------+       +--------+\n  | Sensors | ----> | Kafka  | ---> | Flink  | ---> | Data Warehouse |\n  +---------+       +--------+      +--------+       +--------+\n```\n\n### Example SQL for Stream Processing:\n```sql\nCREATE STREAM real_time_sales (\n    sale_id INT,\n    amount DECIMAL(10,2),\n    sale_timestamp TIMESTAMP\n);\n\nINSERT INTO data_warehouse_sales\nSELECT sale_id, amount, sale_timestamp FROM real_time_sales;\n```\n\n### Case Study:\nTwitter processes billions of real-time user events per day and streams them into Snowflake for near-instant analytics.\n\n### Real-Time Architecture:\nTwitter uses Kafka Streams for real-time data ingestion and Snowflake for storage and analytics, reducing ETL lag.",
    "reference": "https://flink.apache.org/"
  }
]
